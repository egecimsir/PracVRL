{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b9fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4cf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import map_labels_to_trainIds, CityscapesDataset\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, feat_dir, mask_dir, split, timestep: float, feat_transform=None):\n",
    "        assert 0.0 <= timestep <= 1.0\n",
    "        self.feat_dir = os.path.join(feat_dir, split, f\"timestep_{int(timestep*100)}\")\n",
    "        self.mask_dir = os.path.join(mask_dir, \"gtFine\", split)\n",
    "        self.split = split\n",
    "        self.timestep = timestep\n",
    "\n",
    "        ## Get features as tensors\n",
    "        feature_paths = []\n",
    "        for pt_file in os.listdir(self.feat_dir):\n",
    "            path = os.path.join(self.feat_dir, pt_file)\n",
    "            feature_paths.append(path)\n",
    "\n",
    "        features = []\n",
    "        for path in feature_paths:\n",
    "            batch = torch.load(path)\n",
    "            features.append(batch)\n",
    "\n",
    "        self.features = torch.cat(features, dim=0)\n",
    "        del features\n",
    "\n",
    "        ## Get seg_mask paths\n",
    "        mask_paths = []\n",
    "        for city in os.listdir(self.mask_dir):\n",
    "            city_path = os.path.join(self.mask_dir, city)\n",
    "            for fpath in os.listdir(city_path):\n",
    "                if \"labelIds\" in fpath:\n",
    "                    lbl_path = os.path.join(city_path, fpath)\n",
    "                    mask_paths.append(lbl_path)\n",
    "\n",
    "        mask_paths = sorted(mask_paths)\n",
    "\n",
    "        assert len(mask_paths) == len(self.features), f\"Mismatched files! features={len(self.features)} | masks={len(mask_paths)}\"\n",
    "\n",
    "        self.mask_paths = mask_paths\n",
    "\n",
    "        ## Transforms\n",
    "        self.feat_transform = feat_transform\n",
    "        # Modified mask_transform sequence\n",
    "        self.mask_transform = T.Compose([\n",
    "            T.PILToTensor(),                                        # Convert PIL Image to tensor (1, H, W)\n",
    "            T.Lambda(lambda x: x.squeeze(0).long()),                # Remove channel dim and convert to long (H, W)\n",
    "            T.Lambda(lambda x: map_labels_to_trainIds(x, CityscapesDataset.label2trainId)), # Map labels (0-18, 19 for ignore)\n",
    "            # Set ignore_index (19) to -1 for CrossEntropyLoss\n",
    "            T.Lambda(lambda x: torch.where(x == 19, torch.tensor(-1, dtype=torch.long), x)),\n",
    "            T.Resize((224, 224), interpolation=T.InterpolationMode.NEAREST), # Resize with nearest neighbor\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features[idx]\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "\n",
    "        if self.feat_transform is not None:\n",
    "            feat = self.feat_transform(feat)\n",
    "\n",
    "        # Apply the modified mask_transform\n",
    "        mask = self.mask_transform(mask)\n",
    "\n",
    "        return feat, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acf39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Model ----------------\n",
    "class SimpleSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_dim=1152, n_class=19, out_size=224):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # Reshape flat vector to feature map\n",
    "        self.feature_dim = (128, 3, 3)  # 128×3×3 = 1152\n",
    "\n",
    "        # Convolutional decoder to upsample to 224×224\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 6×6\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),    # 12×12\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 24×24\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # 48×48\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, n_class, kernel_size=4, stride=2, padding=1), # 96×96\n",
    "            nn.Upsample(size=(out_size, out_size), mode='bilinear', align_corners=False)  # final 224×224\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1152)\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, *self.feature_dim)  # (B, 128, 3, 3)\n",
    "        logits = self.decoder(x)          # (B, n_class, 224, 224)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2221cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Metrics ----------------\n",
    "def compute_iou(preds, labels, num_classes, device):\n",
    "    ious = []\n",
    "    # Ensure labels are on the same device as preds before comparison\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1).to(device) # Move labels to device\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = preds == cls\n",
    "        target_inds = labels == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # or append 0.0\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return ious\n",
    "\n",
    "\n",
    "def pixel_accuracy(preds, labels):\n",
    "    # Ensure labels are on the same device as preds before comparison\n",
    "    correct = (preds == labels.to(preds.device)).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        # Ensure masks are long type for CrossEntropyLoss\n",
    "        masks = masks.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# ---------------- Evaluation Loop ----------------\n",
    "def evaluate(model, dataloader, criterion, device, num_classes=19):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou = []\n",
    "    total_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            # Ensure masks are long type for CrossEntropyLoss\n",
    "            masks = masks.to(device).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # Pass device to compute_iou and pixel_accuracy\n",
    "            ious = compute_iou(preds, masks, num_classes, device)\n",
    "            acc = pixel_accuracy(preds, masks)\n",
    "\n",
    "            total_iou.append(ious)\n",
    "            total_acc.append(acc)\n",
    "\n",
    "    mean_iou = np.nanmean(np.array(total_iou), axis=0)\n",
    "    overall_acc = np.mean(total_acc)\n",
    "    return total_loss / len(dataloader), mean_iou, overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44258a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/21020/ipykernel_1351558/1087024842.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(path)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatched files! features=640 | masks=2975",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m## Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m trainset = \u001b[43mFeatureDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFEAT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m testset = FeatureDataset(FEAT_DIR, DATA_DIR, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, timestep=\u001b[32m0.95\u001b[39m)\n\u001b[32m     18\u001b[39m model = SimpleSegmentationHead().to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mFeatureDataset.__init__\u001b[39m\u001b[34m(self, feat_dir, mask_dir, split, timestep, feat_transform)\u001b[39m\n\u001b[32m     33\u001b[39m             mask_paths.append(lbl_path)\n\u001b[32m     35\u001b[39m mask_paths = \u001b[38;5;28msorted\u001b[39m(mask_paths)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mask_paths) == \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.features), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatched files! features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | masks=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mask_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.mask_paths = mask_paths\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m## Transforms\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: Mismatched files! features=640 | masks=2975"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# Define training parameters\n",
    "DATA_DIR = \"cityscapes\"\n",
    "FEAT_DIR = \"cityscapes_features\"\n",
    "\n",
    "EPOCHS = 10\n",
    "L_RATE = 0.001\n",
    "B_SIZE = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "## Load data\n",
    "trainset = FeatureDataset(FEAT_DIR, DATA_DIR, split=\"train\", timestep=0.95)\n",
    "testset = FeatureDataset(FEAT_DIR, DATA_DIR, split=\"test\", timestep=0.95)\n",
    "\n",
    "model = SimpleSegmentationHead().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), L_RATE)\n",
    "# Set ignore_index to -1 to ignore those values in the loss calculation\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=B_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss = train(model, train_loader, optim, loss_fn, device)\n",
    "    val_loss, mean_iou, val_acc = evaluate(model, test_loader, loss_fn, device, 19)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Pixel Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Val Mean IoU: {np.nanmean(mean_iou):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e154b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
